#!/usr/bin/env python3
# main_1.py - ç¬¬ä¸€é˜¶æ®µï¼šæ•°æ®é‡‡é›†ä¸æ¸…æ´—æ¨¡å—é›†æˆ
# =============================================================================
# æ ¸å¿ƒåŠŸèƒ½ï¼š
# 1. å¼‚æ­¥æ•°æ®é‡‡é›†ï¼ˆBinance/Deribitï¼‰
# 2. æ•°æ®æ¸…æ´—å’Œæ ‡å‡†åŒ–
# 3. æ•°æ®å­˜å‚¨åˆ°æ•°æ®åº“
# 4. å®šæ—¶ä»»åŠ¡è°ƒåº¦
# =============================================================================

import asyncio
import signal
import sys
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
import pandas as pd
import schedule
import time
from pathlib import Path

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from logger import TradingLogger
from utils import ConfigManager, TimeUtils
from database_manager import DatabaseManager
from data_collector import MarketDataCollector, DataSource
from data_cleaner import DataCleaner


class DataPipeline:
    """ç¬¬ä¸€é˜¶æ®µæ•°æ®ç®¡é“"""
    
    def __init__(self, config_path: str = "config.yaml"):
        self.config = ConfigManager(config_path)
        self.logger = TradingLogger().get_logger("DATA_PIPELINE")
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.db_manager = DatabaseManager(config_path)
        self.data_collector = MarketDataCollector(config_path)
        self.data_cleaner = DataCleaner(config_path)
        
        # è¿è¡ŒçŠ¶æ€
        self.is_running = False
        self.collection_task = None
        
        # æ•°æ®ç¼“å†²åŒº
        self.data_buffer = {
            'ticker': [],
            'trade': [],
            'depth': []
        }
        self.buffer_size = self.config.get("pipeline.buffer_size", 1000)
        self.flush_interval = self.config.get("pipeline.flush_interval", 60)
        
        # é…ç½®
        self.symbols = self.config.get("trading.symbols", ["BTCUSDT", "ETHUSDT"])
        self.exchanges = [DataSource.BINANCE]  # å¯æ‰©å±•åˆ°å…¶ä»–äº¤æ˜“æ‰€
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_received': 0,
            'total_cleaned': 0,
            'total_stored': 0,
            'errors': 0,
            'last_update': TimeUtils.now_timestamp()
        }
        
        self.logger.info("æ•°æ®ç®¡é“åˆå§‹åŒ–å®Œæˆ")
    
    async def start(self):
        """å¯åŠ¨æ•°æ®ç®¡é“"""
        try:
            self.logger.info("å¯åŠ¨æ•°æ®ç®¡é“...")
            self.is_running = True
            
            # æ³¨å†Œæ•°æ®å›è°ƒ
            self.data_collector.add_data_callback(self._handle_market_data)
            
            # å¯åŠ¨æ•°æ®é‡‡é›†
            await self.data_collector.start(self.symbols, self.exchanges)
            
            # å¯åŠ¨å®šæ—¶ä»»åŠ¡
            self._start_scheduled_tasks()
            
            # å¯åŠ¨ç¼“å†²åŒºåˆ·æ–°ä»»åŠ¡
            self.collection_task = asyncio.create_task(self._buffer_flush_loop())
            
            self.logger.info(f"æ•°æ®ç®¡é“å·²å¯åŠ¨ï¼Œç›‘æ§äº¤æ˜“å¯¹: {self.symbols}")
            
        except Exception as e:
            self.logger.error(f"å¯åŠ¨æ•°æ®ç®¡é“å¤±è´¥: {e}")
            raise
    
    async def stop(self):
        """åœæ­¢æ•°æ®ç®¡é“"""
        self.logger.info("åœæ­¢æ•°æ®ç®¡é“...")
        self.is_running = False
        
        # åœæ­¢æ•°æ®é‡‡é›†
        await self.data_collector.stop()
        
        # åˆ·æ–°å‰©ä½™æ•°æ®
        await self._flush_buffers()
        
        # å–æ¶ˆä»»åŠ¡
        if self.collection_task:
            self.collection_task.cancel()
        
        # å…³é—­æ•°æ®åº“è¿æ¥
        self.db_manager.close()
        
        self.logger.info("æ•°æ®ç®¡é“å·²åœæ­¢")
    
    async def _handle_market_data(self, data_type: str, data: Dict[str, Any]):
        """å¤„ç†æ¥æ”¶åˆ°çš„å¸‚åœºæ•°æ®"""
        try:
            self.stats['total_received'] += 1
            
            # æ·»åŠ åˆ°ç¼“å†²åŒº
            if data_type in self.data_buffer:
                self.data_buffer[data_type].append(data)
                
                # æ£€æŸ¥ç¼“å†²åŒºå¤§å°
                if len(self.data_buffer[data_type]) >= self.buffer_size:
                    await self._process_buffer(data_type)
            
        except Exception as e:
            self.logger.error(f"å¤„ç†å¸‚åœºæ•°æ®å¤±è´¥: {e}")
            self.stats['errors'] += 1
    
    async def _process_buffer(self, data_type: str):
        """å¤„ç†ç¼“å†²åŒºæ•°æ®"""
        if not self.data_buffer[data_type]:
            return
        
        try:
            # è½¬æ¢ä¸ºDataFrame
            df = pd.DataFrame(self.data_buffer[data_type])
            
            # æŒ‰äº¤æ˜“å¯¹åˆ†ç»„å¤„ç†
            for symbol in df['symbol'].unique():
                symbol_data = df[df['symbol'] == symbol].copy()
                
                # æ•°æ®æ¸…æ´—
                if data_type == 'ticker':
                    cleaned_data = await self._clean_ticker_data(symbol_data, symbol)
                elif data_type == 'trade':
                    cleaned_data = await self._clean_trade_data(symbol_data, symbol)
                elif data_type == 'depth':
                    cleaned_data = await self._clean_depth_data(symbol_data, symbol)
                else:
                    continue
                
                # å­˜å‚¨åˆ°æ•°æ®åº“
                if cleaned_data is not None and not cleaned_data.empty:
                    await self._store_data(cleaned_data, data_type, symbol)
            
            # æ¸…ç©ºç¼“å†²åŒº
            self.data_buffer[data_type] = []
            
        except Exception as e:
            self.logger.error(f"å¤„ç†ç¼“å†²åŒºå¤±è´¥ [{data_type}]: {e}")
            self.stats['errors'] += 1
    
    async def _clean_ticker_data(self, df: pd.DataFrame, symbol: str) -> Optional[pd.DataFrame]:
        """æ¸…æ´—tickeræ•°æ®"""
        try:
            # é‡å‘½ååˆ—ä»¥åŒ¹é…æ¸…æ´—å™¨æœŸæœ›çš„æ ¼å¼
            df_clean = df.rename(columns={
                'open': 'open',
                'high': 'high',
                'low': 'low',
                'close': 'close',
                'volume': 'volume'
            })
            
            # æ‰§è¡Œæ¸…æ´—
            cleaned_data, quality_report = self.data_cleaner.clean_market_data(
                df_clean,
                symbol,
                detect_outliers=True,
                handle_missing=True,
                normalize_data=False  # åŸå§‹æ•°æ®ä¸æ ‡å‡†åŒ–
            )
            
            self.stats['total_cleaned'] += len(cleaned_data)
            
            # è®°å½•æ•°æ®è´¨é‡
            if quality_report['data_quality_score'] < 0.8:
                self.logger.warning(
                    f"{symbol} æ•°æ®è´¨é‡è¾ƒä½: {quality_report['data_quality_score']:.2f}"
                )
            
            return cleaned_data
            
        except Exception as e:
            self.logger.error(f"æ¸…æ´—tickeræ•°æ®å¤±è´¥: {e}")
            return None
    
    async def _clean_trade_data(self, df: pd.DataFrame, symbol: str) -> Optional[pd.DataFrame]:
        """æ¸…æ´—äº¤æ˜“æ•°æ®"""
        try:
            # åŸºç¡€æ¸…æ´—
            df_clean = df.copy()
            
            # ç§»é™¤é‡å¤æ•°æ®
            df_clean = df_clean.drop_duplicates(subset=['timestamp'], keep='last')
            
            # ç§»é™¤æ— æ•ˆä»·æ ¼
            df_clean = df_clean[
                (df_clean['price'] > 0) & 
                (df_clean['quantity'] > 0)
            ]
            
            # æŒ‰æ—¶é—´æ’åº
            df_clean = df_clean.sort_values('timestamp')
            
            self.stats['total_cleaned'] += len(df_clean)
            
            return df_clean
            
        except Exception as e:
            self.logger.error(f"æ¸…æ´—tradeæ•°æ®å¤±è´¥: {e}")
            return None
    
    async def _clean_depth_data(self, df: pd.DataFrame, symbol: str) -> Optional[pd.DataFrame]:
        """æ¸…æ´—æ·±åº¦æ•°æ®"""
        try:
            df_clean = df.copy()
            
            # ç§»é™¤æ— æ•ˆæ•°æ®
            df_clean = df_clean[
                (df_clean['bid_price'] > 0) & 
                (df_clean['ask_price'] > 0) &
                (df_clean['bid_price'] < df_clean['ask_price'])  # ä¹°ä»·å¿…é¡»å°äºå–ä»·
            ]
            
            # ç§»é™¤é‡å¤æ—¶é—´æˆ³
            df_clean = df_clean.drop_duplicates(subset=['timestamp'], keep='last')
            
            self.stats['total_cleaned'] += len(df_clean)
            
            return df_clean
            
        except Exception as e:
            self.logger.error(f"æ¸…æ´—depthæ•°æ®å¤±è´¥: {e}")
            return None
    
    async def _store_data(self, df: pd.DataFrame, data_type: str, symbol: str):
        """å­˜å‚¨æ•°æ®åˆ°æ•°æ®åº“"""
        try:
            if data_type == 'ticker':
                # å­˜å‚¨OHLCVæ•°æ®
                for _, row in df.iterrows():
                    await self.db_manager.write_market_data(
                        symbol=symbol,
                        timestamp=int(row['timestamp']),
                        ohlcv={
                            'open': row['open'],
                            'high': row['high'],
                            'low': row['low'],
                            'close': row['close'],
                            'volume': row['volume']
                        }
                    )
            
            elif data_type == 'trade':
                # å­˜å‚¨äº¤æ˜“æ•°æ®
                for _, row in df.iterrows():
                    await self.db_manager.db.write_trade_data(
                        symbol=symbol,
                        timestamp=int(row['timestamp']),
                        price=row['price'],
                        quantity=row['quantity'],
                        is_buyer_maker=row.get('is_buyer_maker', False)
                    )
            
            elif data_type == 'depth':
                # å­˜å‚¨æ·±åº¦æ•°æ®
                for _, row in df.iterrows():
                    # è¿™é‡Œå¯ä»¥æ‰©å±•database_manageræ·»åŠ depthæ•°æ®å­˜å‚¨æ–¹æ³•
                    pass
            
            self.stats['total_stored'] += len(df)
            self.logger.debug(f"å­˜å‚¨ {len(df)} æ¡ {data_type} æ•°æ® [{symbol}]")
            
        except Exception as e:
            self.logger.error(f"å­˜å‚¨æ•°æ®å¤±è´¥: {e}")
            self.stats['errors'] += 1
    
    async def _flush_buffers(self):
        """åˆ·æ–°æ‰€æœ‰ç¼“å†²åŒº"""
        self.logger.info("åˆ·æ–°æ•°æ®ç¼“å†²åŒº...")
        
        for data_type in self.data_buffer:
            if self.data_buffer[data_type]:
                await self._process_buffer(data_type)
    
    async def _buffer_flush_loop(self):
        """å®šæœŸåˆ·æ–°ç¼“å†²åŒº"""
        while self.is_running:
            try:
                await asyncio.sleep(self.flush_interval)
                await self._flush_buffers()
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                self.logger.error(f"ç¼“å†²åŒºåˆ·æ–°å¾ªç¯é”™è¯¯: {e}")
    
    def _start_scheduled_tasks(self):
        """å¯åŠ¨å®šæ—¶ä»»åŠ¡"""
        # æ¯å°æ—¶ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        schedule.every().hour.do(self._generate_hourly_report)
        
        # æ¯å¤©æ¸…ç†å†å²æ•°æ®
        schedule.every().day.at("00:00").do(self._cleanup_old_data)
        
        # æ¯30åˆ†é’Ÿæ£€æŸ¥æ•°æ®è´¨é‡
        schedule.every(30).minutes.do(self._check_data_quality)
        
        # å¯åŠ¨è°ƒåº¦çº¿ç¨‹
        asyncio.create_task(self._run_schedule())
    
    async def _run_schedule(self):
        """è¿è¡Œè°ƒåº¦ä»»åŠ¡"""
        while self.is_running:
            try:
                schedule.run_pending()
                await asyncio.sleep(1)
            except Exception as e:
                self.logger.error(f"è°ƒåº¦ä»»åŠ¡é”™è¯¯: {e}")
    
    def _generate_hourly_report(self):
        """ç”Ÿæˆå°æ—¶æŠ¥å‘Š"""
        try:
            report = {
                'timestamp': TimeUtils.now_timestamp(),
                'total_received': self.stats['total_received'],
                'total_cleaned': self.stats['total_cleaned'],
                'total_stored': self.stats['total_stored'],
                'errors': self.stats['errors'],
                'success_rate': (
                    (self.stats['total_stored'] / self.stats['total_received'] * 100)
                    if self.stats['total_received'] > 0 else 0
                )
            }
            
            self.logger.info(
                f"ğŸ“Š å°æ—¶æŠ¥å‘Š - "
                f"æ¥æ”¶: {report['total_received']}, "
                f"æ¸…æ´—: {report['total_cleaned']}, "
                f"å­˜å‚¨: {report['total_stored']}, "
                f"æˆåŠŸç‡: {report['success_rate']:.2f}%"
            )
            
            # é‡ç½®è®¡æ•°å™¨
            self.stats.update({
                'total_received': 0,
                'total_cleaned': 0,
                'total_stored': 0,
                'errors': 0,
                'last_update': TimeUtils.now_timestamp()
            })
            
        except Exception as e:
            self.logger.error(f"ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")
    
    def _cleanup_old_data(self):
        """æ¸…ç†å†å²æ•°æ®"""
        try:
            retention_days = self.config.get("database.retention_days", 30)
            self.logger.info(f"æ¸…ç† {retention_days} å¤©å‰çš„å†å²æ•°æ®...")
            
            # è¿™é‡Œå¯ä»¥è°ƒç”¨æ•°æ®åº“ç®¡ç†å™¨çš„æ¸…ç†æ–¹æ³•
            # self.db_manager.cleanup_old_data(retention_days)
            
        except Exception as e:
            self.logger.error(f"æ¸…ç†å†å²æ•°æ®å¤±è´¥: {e}")
    
    def _check_data_quality(self):
        """æ£€æŸ¥æ•°æ®è´¨é‡"""
        try:
            # æ£€æŸ¥æœ€è¿‘çš„æ•°æ®è´¨é‡
            for symbol in self.symbols:
                # è¿™é‡Œå¯ä»¥å®ç°æ•°æ®è´¨é‡æ£€æŸ¥é€»è¾‘
                pass
                
        except Exception as e:
            self.logger.error(f"æ£€æŸ¥æ•°æ®è´¨é‡å¤±è´¥: {e}")
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats.copy()


async def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºæ•°æ®ç®¡é“
    pipeline = DataPipeline("config.yaml")
    
    # è®¾ç½®ä¿¡å·å¤„ç†
    def signal_handler(sig, frame):
        print("\næ­£åœ¨åœæ­¢æ•°æ®ç®¡é“...")
        asyncio.create_task(pipeline.stop())
        sys.exit(0)
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    try:
        # å¯åŠ¨ç®¡é“
        await pipeline.start()
        
        # ä¿æŒè¿è¡Œ
        while pipeline.is_running:
            await asyncio.sleep(10)
            
            # æ‰“å°çŠ¶æ€
            stats = pipeline.get_stats()
            print(f"\rğŸ“ˆ çŠ¶æ€ - æ¥æ”¶: {stats['total_received']}, "
                  f"å­˜å‚¨: {stats['total_stored']}, "
                  f"é”™è¯¯: {stats['errors']}", end='')
    
    except KeyboardInterrupt:
        print("\næ¥æ”¶åˆ°ä¸­æ–­ä¿¡å·")
    except Exception as e:
        print(f"\né”™è¯¯: {e}")
    finally:
        await pipeline.stop()


if __name__ == "__main__":
    # ç¡®ä¿é…ç½®æ–‡ä»¶å­˜åœ¨
    config_path = Path("config.yaml")
    if not config_path.exists():
        # åˆ›å»ºé»˜è®¤é…ç½®
        default_config = """
database:
  type: influxdb
  host: localhost
  port: 8086
  database: ml2025_trading
  username: ""
  password: ""
  retention_days: 30

trading:
  symbols:
    - BTCUSDT
    - ETHUSDT
  exchanges:
    - binance

collector:
  max_reconnect_attempts: 10
  reconnect_delay: 5
  heartbeat_interval: 30
  rest_poll_interval: 60

network:
  proxy_enabled: true
  proxy_http: "http://127.0.0.1:7890"
  proxy_https: "http://127.0.0.1:7890"

cleaner:
  outlier_method: modified_zscore
  outlier_threshold: 3.5
  imputation_method: linear
  scaling_method: robust
  outlier_strategy: cap

pipeline:
  buffer_size: 1000
  flush_interval: 60

logging:
  level: INFO
  max_bytes: 10485760
  backup_count: 5
"""
        config_path.write_text(default_config)
        print("å·²åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶ config.yaml")
    
    # è¿è¡Œä¸»ç¨‹åº
    asyncio.run(main())